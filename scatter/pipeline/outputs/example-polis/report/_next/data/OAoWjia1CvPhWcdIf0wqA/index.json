{"pageProps":{"result":{"clusters":[{"cluster":"環境への配慮と公平性を重視すべき","cluster_id":"0","takeaways":"AIの利点とリスクは公平に分配されるべきであり、少数のグループに集中すべきではありません。AIは世界中の民主主義に従うべきであり、人間の監督なしに重要な環境決定を自動化または制御すべきではありません。変革的なAIアプリケーションの開発と展開には、速さではなく慎重さと注意が導くべきであり、AI開発はイノベーションを促進し、地理的または経済的な障壁に関係なく利点が全てにアクセス可能であることを確認すべきです。","arguments":[{"arg_id":"A321_0","argument":"AIシステムの利点とリスクは公平に分配すべきであり、少数のグループに集中すべきではない","comment_id":"321","x":2.0786,"y":7.7366,"p":200},{"arg_id":"A341_1","argument":"AIは世界中の民主主義に沿うべきだ","comment_id":"341","x":2.6779,"y":6.8509,"p":200},{"arg_id":"A339_0","argument":"AIは人間の監督なしに重要な環境決定を自動化または制御すべきではない","comment_id":"339","x":1.9185,"y":8.3581,"p":200},{"arg_id":"A337_0","argument":"変革的なAIアプリケーションの開発と展開には、速さではなく慎重さと注意が導くべき","comment_id":"337","x":2.6239,"y":8.4237,"p":200},{"arg_id":"A332_0","argument":"AI開発はイノベーションを促進し、地理的または経済的な障壁に関係なく利点が全てにアクセス可能であることを確認すべき","comment_id":"332","x":2.8535,"y":7.6235,"p":200}]},{"cluster":"AIの持続可能性と環境配慮","cluster_id":"1","takeaways":"AIのガバナンスフレームワークは民主主義の原則に従うべきですが、それが西洋の民主主義に限定されるべきではありません。環境の代表者を含めるべきであり、生態系への影響を考慮すべきです。AIの研究者や開発者は、自らの仕事の環境への影響を倫理的に考慮する責任があります。","arguments":[{"arg_id":"A336_0","argument":"AI governance frameworks must include environmental representatives","comment_id":"336","x":1.2447,"y":6.781,"p":200},{"arg_id":"A336_1","argument":"AI governance frameworks must consider ecological impacts","comment_id":"336","x":1.0819,"y":6.1901,"p":200},{"arg_id":"A335_0","argument":"AI researchers and developers have an ethical responsibility to consider the environmental impacts of their work.","comment_id":"335","x":0.5454,"y":6.6463,"p":200},{"arg_id":"A341_0","argument":"AIは民主主義の原則に沿うべきだが、西洋の民主主義に限定されるべきではない","comment_id":"341","x":2.0325,"y":6.8757,"p":0}]},{"cluster":"環境への配慮","cluster_id":"2","takeaways":"AIと環境に関する公衆認識、教育、参加を向上する必要があります。AIシステムは効率と利益だけでなく、正義、エンパワーメント、環境保護を促進すべきです。具体的には、AIを活用して人類の環境負荷を監視し、モデル化し、最終的に削減する必要があります。また、AIシステムの環境フットプリントを理解するために包括的なライフサイクルアセスメントを実施し、持続可能性を考慮して設計・展開すべきです。エネルギー利用や材料・廃棄物などの環境影響も考慮すべきです。","arguments":[{"arg_id":"A338_0","argument":"AIシステムは効率と利益だけでなく、正義、エンパワーメント、環境保護を促進すべき","comment_id":"338","x":1.3985,"y":8.3627,"p":200},{"arg_id":"A334_0","argument":"AIを活用して人類の環境負荷を監視し、モデル化し、最終的に削減するべき","comment_id":"334","x":0.6794,"y":8.8208,"p":200},{"arg_id":"A333_0","argument":"AIシステムの環境フットプリントを理解するために包括的なライフサイクルアセスメントを実施すべき","comment_id":"333","x":0.3332,"y":7.567,"p":200},{"arg_id":"A331_0","argument":"AIシステムは持続可能性を考慮して設計・展開すべき","comment_id":"331","x":0.646,"y":8.1371,"p":200},{"arg_id":"A340_0","argument":"AIと環境に関する公衆認識、教育、参加を向上する必要がある","comment_id":"340","x":1.2251,"y":7.5887,"p":100},{"arg_id":"A331_1","argument":"エネルギー利用や材料・廃棄物などの環境影響を考慮すべき","comment_id":"331","x":0.056,"y":8.123,"p":100}]}],"comments":{"":{}},"translations":{},"overview":"クラスター0/3では、AIの利点とリスクの公平な分配が重要であり、環境への配慮と公平性が強調されています。クラスター1/3では、AIの持続可能性と環境配慮が重要であり、環境の代表者を含めるべきだと述べられています。クラスター2/3では、AIを活用して環境負荷を監視し、削減する必要があり、環境フットプリントを理解するための取り組みが必要とされています。","config":{"name":"Recursive Public, Agenda Setting","question":"人類が人工知能を開発・展開する上で、最優先すべき課題は何でしょうか？","input":"example-polis","model":"gpt-3.5-turbo","extraction":{"workers":3,"limit":12,"source_code":"import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    existing_arguments = set()\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                if arg not in existing_arguments:\n                    new_row = {\n                        \"arg-id\": f\"A{comment_id}_{j}\",\n                        \"comment-id\": int(comment_id),\n                        \"argument\": arg,\n                    }\n                    results = pd.concat(\n                        [results, pd.DataFrame([new_row])], ignore_index=True\n                    )\n                    existing_arguments.add(arg)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [\n            executor.submit(extract_arguments, input, prompt, model)\n            for input in list(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait(futures, timeout=30)\n\n        results = []\n\n        for future in not_done:\n            if not future.cancelled():\n                future.cancel()\n            results.append([])\n\n        for future in done:\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                logging.error(f\"Task {future} failed with error: {e}\")\n                results.append([])\n\n        return results\n\n\ndef extract_by_llm(input, prompt, model):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model, is_json=False)\n        response = (\n            COMMA_AND_SPACE_AND_RIGHT_BRACKET.sub(r\"\\1\", response)\n            .replace(\"```json\", \"\")\n            .replace(\"```\", \"\")\n        )\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        try:\n            items = [a.strip() for a in obj]\n        except Exception as e:\n            print(\"Error:\", e)\n            print(\"Input was:\", input)\n            print(\"Response was:\", response)\n            print(\"JSON was:\", obj)\n            print(\"skip\")\n            items = []\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []\n","prompt":"/system\nあなたは専門的なリサーチアシスタントで、整理された議論のデータセットを作成するお手伝いをする役割です。\n人工知能に関する公開協議を実施した状況を想定しています。一般市民から寄せられた議論の例を提示しますので、それらをより簡潔で読みやすい形に整理するお手伝いをお願いします。必要な場合は2つの別個の議論に分割することもできますが、多くの場合は1つの議論にまとめる方が望ましいでしょう。\n結果は整形されたJSON形式の文字列リストとして返してください。\n要約は必ず日本語で作成してください。\n\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n[\n\"AIテクノロジーの環境負荷削減に焦点を当てるべき\"\n]\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、一般市民を教育する協調的な取り組みが必要です。\n\n/ai\n\n[\n\"AIの能力について一般市民を教育すべき\",\n\"AIの限界と倫理的考慮事項について一般市民を教育すべき\"\n]\n\n/human\n\nAIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できます。\n\n/ai\n\n[\n\"AIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できる\"\n]\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n[\n\"AIはエネルギーグリッドを最適化して無駄と炭素排出を削減できる\"\n]","model":"gpt-3.5-turbo"},"clustering":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nfrom janome.tokenizer import Tokenizer\n\nSTOP_WORDS = [\n    \"の\",\n    \"に\",\n    \"は\",\n    \"を\",\n    \"た\",\n    \"が\",\n    \"で\",\n    \"て\",\n    \"と\",\n    \"し\",\n    \"れ\",\n    \"さ\",\n    \"ある\",\n    \"いる\",\n    \"も\",\n    \"する\",\n    \"から\",\n    \"な\",\n    \"こと\",\n    \"として\",\n    \"いく\",\n    \"ない\",\n]\nTOKENIZER = Tokenizer()\n\n\ndef clustering(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config[\"clustering\"][\"clusters\"]\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        min_cluster_size=clusters,\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef tokenize_japanese(text):\n    return [\n        token.surface\n        for token in TOKENIZER.tokenize(text)\n        if token.surface not in STOP_WORDS\n    ]\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module(\"sklearn.cluster\").SpectralClustering\n    HDBSCAN = import_module(\"hdbscan\").HDBSCAN\n    UMAP = import_module(\"umap\").UMAP\n    CountVectorizer = import_module(\"sklearn.feature_extraction.text\").CountVectorizer\n    BERTopic = import_module(\"bertopic\").BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    vectorizer_model = CountVectorizer(tokenizer=tokenize_japanese)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42,\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[[\"arg-id\", \"x\", \"y\", \"probability\"]]\n    result[\"cluster-id\"] = cluster_labels\n\n    return result\n"},"intro":"このAI生成レポートは、Recursive Publicチームが実施したPolis協議のデータに基づいています。\n分析対象となったデータの件数は342件で、これらのデータに対してOpenAI APIを用いて15件の意見（議論）を抽出し、クラスタリングを行った。\n一部、AIによる分析結果の中で、事実と異なる内容については削除を行った。","output_dir":"example-polis","previous":{"name":"Recursive Public, Agenda Setting","question":"人類が人工知能を開発・展開する上で、最優先すべき課題は何でしょうか？","input":"example-polis","model":"gpt-3.5-turbo","extraction":{"workers":3,"limit":12,"source_code":"import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    existing_arguments = set()\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                if arg not in existing_arguments:\n                    new_row = {\n                        \"arg-id\": f\"A{comment_id}_{j}\",\n                        \"comment-id\": int(comment_id),\n                        \"argument\": arg,\n                    }\n                    results = pd.concat(\n                        [results, pd.DataFrame([new_row])], ignore_index=True\n                    )\n                    existing_arguments.add(arg)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [\n            executor.submit(extract_arguments, input, prompt, model)\n            for input in list(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait(futures, timeout=30)\n\n        results = []\n\n        for future in not_done:\n            if not future.cancelled():\n                future.cancel()\n            results.append([])\n\n        for future in done:\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                logging.error(f\"Task {future} failed with error: {e}\")\n                results.append([])\n\n        return results\n\n\ndef extract_by_llm(input, prompt, model):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model, is_json=False)\n        response = (\n            COMMA_AND_SPACE_AND_RIGHT_BRACKET.sub(r\"\\1\", response)\n            .replace(\"```json\", \"\")\n            .replace(\"```\", \"\")\n        )\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        try:\n            items = [a.strip() for a in obj]\n        except Exception as e:\n            print(\"Error:\", e)\n            print(\"Input was:\", input)\n            print(\"Response was:\", response)\n            print(\"JSON was:\", obj)\n            print(\"skip\")\n            items = []\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []\n","prompt":"/system\nあなたは専門的なリサーチアシスタントで、整理された議論のデータセットを作成するお手伝いをする役割です。\n人工知能に関する公開協議を実施した状況を想定しています。一般市民から寄せられた議論の例を提示しますので、それらをより簡潔で読みやすい形に整理するお手伝いをお願いします。必要な場合は2つの別個の議論に分割することもできますが、多くの場合は1つの議論にまとめる方が望ましいでしょう。\n結果は整形されたJSON形式の文字列リストとして返してください。\n要約は必ず日本語で作成してください。\n\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n[\n\"AIテクノロジーの環境負荷削減に焦点を当てるべき\"\n]\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、一般市民を教育する協調的な取り組みが必要です。\n\n/ai\n\n[\n\"AIの能力について一般市民を教育すべき\",\n\"AIの限界と倫理的考慮事項について一般市民を教育すべき\"\n]\n\n/human\n\nAIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できます。\n\n/ai\n\n[\n\"AIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できる\"\n]\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n[\n\"AIはエネルギーグリッドを最適化して無駄と炭素排出を削減できる\"\n]","model":"gpt-3.5-turbo"},"clustering":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nfrom janome.tokenizer import Tokenizer\n\nSTOP_WORDS = [\n    \"の\",\n    \"に\",\n    \"は\",\n    \"を\",\n    \"た\",\n    \"が\",\n    \"で\",\n    \"て\",\n    \"と\",\n    \"し\",\n    \"れ\",\n    \"さ\",\n    \"ある\",\n    \"いる\",\n    \"も\",\n    \"する\",\n    \"から\",\n    \"な\",\n    \"こと\",\n    \"として\",\n    \"いく\",\n    \"ない\",\n]\nTOKENIZER = Tokenizer()\n\n\ndef clustering(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config[\"clustering\"][\"clusters\"]\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        min_cluster_size=clusters,\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef tokenize_japanese(text):\n    return [\n        token.surface\n        for token in TOKENIZER.tokenize(text)\n        if token.surface not in STOP_WORDS\n    ]\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module(\"sklearn.cluster\").SpectralClustering\n    HDBSCAN = import_module(\"hdbscan\").HDBSCAN\n    UMAP = import_module(\"umap\").UMAP\n    CountVectorizer = import_module(\"sklearn.feature_extraction.text\").CountVectorizer\n    BERTopic = import_module(\"bertopic\").BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    vectorizer_model = CountVectorizer(tokenizer=tokenize_japanese)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42,\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[[\"arg-id\", \"x\", \"y\", \"probability\"]]\n    result[\"cluster-id\"] = cluster_labels\n\n    return result\n"},"intro":"このAI生成レポートは、Recursive Publicチームが実施したPolis協議のデータに基づいています。","output_dir":"example-polis","embedding":{"source_code":"import os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom tqdm import tqdm\n\nload_dotenv(\"../../.env\")\n\nEMBEDDING_MODEL = \"text-embedding-3-large\"\n\n\ndef embed_by_openai(args):\n    if os.getenv(\"USE_AZURE\"):\n        embeds = AzureOpenAIEmbeddings(\n            model=EMBEDDING_MODEL,\n            azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n        ).embed_documents(args)\n    else:\n        embeds = OpenAIEmbeddings(model=EMBEDDING_MODEL).embed_documents(args)\n    return embeds\n\n\ndef embedding(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = embed_by_openai(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n# TODO: プロンプト設定の外部化\nBASE_SELECTION_PROMPT = \"\"\"クラスタにつけられたラベル名と、紐づくデータ点のテキストを与えるので、\nラベル名と関連度の高いテキストのidを5つ出力してください\n\n# 指示\n* ラベルと各データ点のテキストを確認した上で、関連度の高いidを出力してください\n* 出力はカンマ区切りで、スペースを含めずに5つのidを出力して下さい\n* 出力結果は人間が閲覧するので、人間が解釈しやすいテキストを選定してください\n    * 出力はWebで公開されるため過激な発言や侮辱的な発言等の閲覧者が不快感を覚えるものは選定しないでください\n    * 同様に公共放送において不適切な単語が含まれているものは選定しないでください\n* 今回の分析は衆院選における意見分析を行うために実施しているため衆院選と関連性の低いものは選定しないでください\n    * データソースはツイートであり、ハッシュタグのみのツイート等も含まれるため、それらは選定しないでください\n\n# 出力例\nA199_0,A308_0,A134_2,A134_1,A123_0\n\n# ラベル名\n{label}\n\n# 各データ点のテキスト\n{args_text}\n\"\"\"\n\n\ndef select_relevant_ids_by_llm(prompt, model=\"gpt-4o\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model)\n        selected_ids = response.strip().split(\",\")\n        return [id_str.strip() for id_str in selected_ids]\n    except Exception as e:\n        print(e)\n        return []\n\n\ndef select_representative_args(\n    cluster_args, label, cid, model=\"gpt-4o\", sampling_num=50\n):\n    arg_rows = cluster_args[cluster_args[\"cluster-id\"] == cid].sort_values(\n        by=\"probability\", ascending=False\n    )\n    # hdbscanのクラスタにおける所属確率(probability)が高い順に取得し、代表コメントの候補とする\n    top_rows = arg_rows.head(sampling_num)\n    args_text = \"\\n\".join(\n        [\n            f\"{row['arg-id']}: {row['argument']}\"\n            for _, (_, row) in enumerate(top_rows.iterrows())\n        ]\n    )\n    prompt = BASE_SELECTION_PROMPT.format(label=label, args_text=args_text)\n    selected_ids = select_relevant_ids_by_llm(prompt, model)\n    return selected_ids\n\n\ndef update_cluster_probability(config, arguments, clusters, labels):\n    cluster_args = arguments.merge(clusters, on=\"arg-id\", how=\"left\")\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        selected_ids = select_representative_args(cluster_args, label, cid)\n        for id in selected_ids:\n            mask = cluster_args[\"arg-id\"] == id\n            clusters.loc[mask, \"probability\"] += 100\n    clusters.to_csv(f\"outputs/{config['output_dir']}/clusters.csv\", index=False)\n\n\ndef labelling(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"labelling\"][\"sample_size\"]\n    prompt = config[\"labelling\"][\"prompt\"]\n    model = config[\"labelling\"][\"model\"]\n\n    question = config[\"question\"]\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n\n        args_ids_outside = clusters[clusters[\"cluster-id\"] != cluster_id][\n            \"arg-id\"\n        ].values\n        args_ids_outside = np.random.choice(\n            args_ids_outside,\n            size=min(len(args_ids_outside), sample_size),\n            replace=False,\n        )\n        args_sample_outside = arguments[arguments[\"arg-id\"].isin(args_ids_outside)][\n            \"argument\"\n        ].values\n\n        label = generate_label(\n            question, args_sample, args_sample_outside, prompt, model\n        )\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"label\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n    update_cluster_probability(config, arguments, clusters, results)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    outside = \"\\n * \" + \"\\n * \".join(args_sample_outside)\n    inside = \"\\n * \" + \"\\n * \".join(args_sample)\n    input = (\n        f\"質問:\\n{question}\\n\\n\"\n        + f\"クラスタ外部の意見:\\n{outside}\\n\"\n        + f\"クラスタ内部の意見:\\n{inside}\"\n    )\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n","prompt":"/system \n\nクラスタ分析の結果を与えるので、クラスタにふさわしいラベルを生成してください。議論から出た意見・要望・批判、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えるのでクラスターを要約する1つのカテゴリーラベルを作成してください。\n\n質問からすでに明らかな文脈は含めない（例えば、相談の質問が「フランスでどのような課題に直面しているか」のようなものであれば、クラスターのラベルに「フランスで」と繰り返す必要はない）。\n\n今回の分析トピックは衆議院議員選挙であり、明らかに衆院選と無関係な内容は出力に含めないでください。\nラベルは非常に簡潔でなければならず、クラスターとその外側にある内容を区別するのに十分な正確さでなければならない。\nラベル名は、必ず日本語で記述してください。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定に際してEUはどのような対処をすべきだと思いますか？\n\n関心のあるクラスター以外の提案と要望の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限されないようにすべき。\n * 国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を軽減すべき。\n * 環境基準における協力を維持し、気候変動と闘う努力を向上すべき。\n * 相互医療協定の中断せずに、患者ケアを減らさないようにすべき。\n * Brexit関連の変更により、家族の居住権や市民権の申請を複雑にしないようにすべき。\n * 英国との共同研究機会を維持し、研究の課題に取り組む世界的な取り組みを維持すべき。\n * EUの文化助成プログラムからの除外の影響を減らし、創造的なプロジェクトを制限しないようにすべき。\n * EUの資金提供の喪失の影響を減らし、慈善活動やコミュニティ支援が後退しないようにしてほしい。\n * 消費者保護の弱体化させず、国境を越えた紛争解決にコミットしてほしい。\n * 英国のプロの音楽家のEU諸国ツアーを制限せず、キャリアに影響を与えないでほしい。\n\nクラスター内部での提案の例\n\n * Brexitによるサプライチェーンへの影響をとどめ、企業にとってコスト増と納期遅延を回避すべき。\n * ブレグジットによる市場の変動や投資・退職金の不確実性を減らしてほしい。\n * 新たな関税や通関手続きを考慮し、英国は輸出業者として利益率の低下に対処すべき。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転せずに、雇用を失わないようにしてほしい。\n * 英国は輸入品価格の高騰による生活費の増加に対処してほしい。\n * 英国のハイテク産業への投資を維持し、技術革新と雇用機会を保つべき。\n * 新たなビザ規制による観光客の減少に備え、新たな接客業への刺激策を考えるべき。\n * ポンド価値の下落により購買力が低下に備え、旅費の増加に対処してほしい。\n\n\n/ai \n\n財務上のマイナス影響への対処を考えるべき\n","model":"gpt-3.5-turbo"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n\ndef takeaways(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"takeaways\"][\"sample_size\"]\n    prompt = config[\"takeaways\"][\"prompt\"]\n    model = config[\"takeaways\"][\"model\"]\n\n    model = config.get(\"model_takeaways\", config.get(\"model\", \"gpt3.5-turbo\"))\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"takeaways\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    input = \"\\n\".join(args_sample)\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n","prompt":"/system \n\nあなたはプロのリサーチ・アシスタントで、私の仕事を手伝うことがあなたの仕事です。\nX上のポストのリストを与えるので、それらの主要なポイントを一つか二つの段落でまとめてください。\n応答テキストは、簡潔で、短く、読みやすい文章にしてください。\n出力は必ず日本語で行ってください。\n \n/human\n\n[\n  \"私は銃暴力が我々の社会における深刻な公衆衛生の危機であると強く信じています。\",\n  \"この問題を包括的な銃規制によって緊急に対処する必要があります。\",\n  \"私は全ての銃購入者に対するユニバーサル背景調査の実施を支持しています。\",\n  \"私はアサルト武器と大容量マガジンの禁止に賛成です。\",\n  \"違法な銃の密売を防ぐために、より厳しい規制を求めています。\",\n  \"銃購入の過程で精神的健康評価を必須にすべきだと主張しています。\"\n]\n/ai \n\n参加者たちは、銃暴力が深刻な社会問題であると認識し、これに対処するために包括的な銃規制の導入を強く求めました。具体的には、全ての銃購入者に対する背景調査の義務化や、アサルト武器と大容量マガジンの禁止、違法な銃取引の取り締まり強化、そして銃購入時の精神健康評価を優先的に導入すべきだとしています。\n","model":"gpt-3.5-turbo"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_openai\n\n\ndef overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config[\"overview\"][\"prompt\"]\n    model = config[\"overview\"][\"model\"]\n\n    ids = labels[\"cluster-id\"].to_list()\n    takeaways.set_index(\"cluster-id\", inplace=True)\n    labels.set_index(\"cluster-id\", inplace=True)\n\n    input = \"\"\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id][\"takeaways\"] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n\n    with open(path, \"w\") as file:\n        file.write(response)\n","prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","model":"gpt-3.5-turbo"},"translation":{"languages":[],"flags":[],"source_code":"import json\n\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom tqdm import tqdm\n\nfrom utils import messages\n\nJAPANESE_UI_MAP = {\n    \"Argument\": \"議論\",\n    # \"Original comment\": \"元のコメント\",\n    \"Representative arguments\": \"代表的な議論\",\n    \"Open full-screen map\": \"全画面地図を開く\",\n    \"Back to report\": \"レポートに戻る\",\n    \"Hide labels\": \"ラベルを非表示にする\",\n    \"Show labels\": \"ラベルを表示\",\n    \"Show filters\": \"フィルターを表示\",\n    \"Hide filters\": \"フィルターを非表示\",\n    \"Min. votes\": \"最小投票数\",\n    \"Consensus\": \"コンセンサス\",\n    \"Showing\": \"表示中\",\n    \"arguments\": \"議論\",\n    \"Reset zoom\": \"ズームをリセット\",\n    \"Click anywhere on the map to close this\": \"このメッセージを閉じるには地図のどこかをクリックしてください\",\n    \"Click on the dot for details\": \"詳細を見るには点をクリックしてください\",\n    \"agree\": \"同意する\",\n    \"disagree\": \"同意しない\",\n    \"Language\": \"言語\",\n    \"English\": \"英語\",\n    \"of total\": \"合計\",\n    \"Overview\": \"分析結果の概要\",\n    \"Cluster analysis\": \"クラスター分析\",\n    \"Representative comments\": \"コメント例\",\n    \"Introduction\": \"導入\",\n    \"Clusters\": \"クラスター\",\n    \"Appendix\": \"付録\",\n    \"This report was generated using an AI pipeline that consists of the following steps\": \"このレポートは、以下のステップで構成されるAIパイプラインを使用して生成されました\",\n    \"Step\": \"ステップ\",\n    \"extraction\": \"抽出\",\n    \"show code\": \"コードを表示\",\n    \"hide code\": \"コードを非表示\",\n    \"show prompt\": \"プロンプトを表示\",\n    \"hide prompt\": \"プロンプトを非表示\",\n    \"embedding\": \"埋め込み\",\n    \"clustering\": \"クラスタリング\",\n    \"labelling\": \"ラベリング\",\n    \"takeaways\": \"まとめ\",\n    \"overview\": \"概要\",\n}\n\n\ndef translation(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, \"w\") as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\n        \"Argument\",\n        \"Original comment\",\n        \"Representative arguments\",\n        \"Open full-screen map\",\n        \"Back to report\",\n        \"Hide labels\",\n        \"Show labels\",\n        \"Show filters\",\n        \"Hide filters\",\n        \"Min. votes\",\n        \"Consensus\",\n        \"Showing\",\n        \"arguments\",\n        \"Reset zoom\",\n        \"Click anywhere on the map to close this\",\n        \"Click on the dot for details\",\n        \"agree\",\n        \"disagree\",\n        \"Language\",\n        \"English\",\n        \"arguments\",\n        \"of total\",\n        \"Overview\",\n        \"Cluster analysis\",\n        \"Representative comments\",\n        \"Introduction\",\n        \"Clusters\",\n        \"Appendix\",\n        \"This report was generated using an AI pipeline that consists of the following steps\",\n        \"Step\",\n        \"extraction\",\n        \"show code\",\n        \"hide code\",\n        \"show prompt\",\n        \"hide prompt\",\n        \"embedding\",\n        \"clustering\",\n        \"labelling\",\n        \"takeaways\",\n        \"overview\",\n    ]\n\n    japanese_ui = [JAPANESE_UI_MAP[key] for key in UI_copy]\n    arg_list = (\n        arguments[\"argument\"].to_list()\n        + labels[\"label\"].to_list()\n        + japanese_ui\n        + languages\n    )\n\n    if \"name\" in config:\n        arg_list.append(config[\"name\"])\n    if \"question\" in config:\n        arg_list.append(config[\"question\"])\n\n    prompt_file = config.get(\"translation_prompt\", \"default\")\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config[\"model\"]\n\n    config[\"translation_prompt\"] = prompt\n\n    translations = [\n        translate_lang(arg_list, 10, prompt, lang, model) for lang in languages\n    ]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways[\"takeaways\"].to_list()\n    long_arg_list.append(overview)\n    if \"intro\" in config:\n        long_arg_list.append(config[\"intro\"])\n\n    long_translations = [\n        translate_lang(long_arg_list, 1, prompt, lang, model) for lang in languages\n    ]\n\n    for i, id in enumerate(arg_list):\n        print(\"i, id\", i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i : i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if i < len(parsed):\n                    print(\"Response:\", parsed[i])\n            if len(batch) > 1:\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(\n                    batch[:mid], lang_prompt, model, retries - 1\n                ) + translate_batch(batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nあなたはプロの翻訳者です。\n英語で書かれた単語と文章のリストを受け取ります。\n同じリストを同じ順番で、日本語に翻訳して返してください。ただし、もし文章が日本語で書かれている場合は元の文をそのまま返してください。\n元のリストと同じ長さの文字列の有効なJSONリストを返すようにしてください。\n\n","model":"gpt-3.5-turbo"},"aggregation":{"include_minor":true,"sampling_num":5000,"title_in_map":null,"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\ndef create_custom_intro(config, total_sampled_num: int):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n分析対象となったデータの件数は{input_count}件で、これらのデータに対してOpenAI APIを用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(\n        intro=intro, input_count=input_count, args_count=args_count\n    )\n\n    if total_sampled_num < args_count:\n        extra_intro = \"なお、クラスタ分析には前述の{args_count}件のデータを用いているが、本ページではそのうち{total_sampled_num}件のデータをサンプリングして可視化している。\".format(\n            args_count=args_count, total_sampled_num=total_sampled_num\n        )\n        custom_intro += extra_intro\n    custom_intro += \"一部、AIによる分析結果の中で、事実と異なる内容については削除を行った。\"\n    with open(result_path, \"r\") as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\") as f:\n        json.dump(result, f, indent=2)\n\n\ndef aggregation(config):\n    path = f\"outputs/{config['output_dir']}/result.json\"\n    total_sampling_num = config[\"aggregation\"][\"sampling_num\"]\n    print(\"total sampling num:\", total_sampling_num)\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {\"\": {}},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results[\"translations\"] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index(\"cluster-id\", inplace=True)\n\n    print(\"relevant clusters score\")\n    print(clusters.sort_values(by=\"probability\", ascending=False).head(10))\n\n    clusters[\"x\"] = clusters[\"x\"].astype(float).round(4)\n    clusters[\"y\"] = clusters[\"y\"].astype(float).round(4)\n    clusters[\"probability\"] = clusters[\"probability\"].astype(float).round(1)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results[\"overview\"] = overview\n\n    # クラスタ事に可視化するデータをサンプルする\n    # 各クラスタの件数の全体での比率をもとにサンプルする\n    arguments_num = len(arguments)\n    sample_rate = min(total_sampling_num / arguments_num, 1)\n    total_sampled_num = 0\n\n    sampled_comment_ids = []\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        arg_rows = clusters[clusters[\"cluster-id\"] == cid]\n        c_arg_num = len(arg_rows)\n        sampling_num = int(c_arg_num * sample_rate)\n\n        if (\n            not config[\"aggregation\"][\"include_minor\"]\n            and sampling_num / total_sampling_num < 0.005\n        ):\n            continue\n        print(f\"sampling num: {sampling_num}\", c_arg_num, sample_rate)\n        total_sampled_num += sampling_num\n        arguments_in_cluster = []\n\n        # pickup top 5 for representative comments\n        sorted_rows = arg_rows.sort_values(by=\"probability\", ascending=False)\n        top_5 = sorted_rows.head(5)\n        for _, arg_row in top_5.head(sampling_num).iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n        results[\"clusters\"].append(\n            {\n                \"cluster\": label,\n                \"cluster_id\": str(cid),\n                \"takeaways\": takeaways.loc[cid][\"takeaways\"],\n                \"arguments\": arguments_in_cluster,\n            }\n        )\n\n        # random sampling\n        remaining = sorted_rows.iloc[5:]\n        remaining_sample_size = max(0, sampling_num - 5)\n        random_sample = remaining.sample(\n            n=min(remaining_sample_size, len(remaining)), random_state=42\n        )\n\n        for _, arg_row in random_sample.iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n    create_custom_intro(config, total_sampled_num)\n"},"visualization":{"replacements":[],"source_code":"import subprocess\n\n\ndef visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":false,"reason":"nothing changed"},{"step":"embedding","run":false,"reason":"nothing changed"},{"step":"clustering","run":false,"reason":"nothing changed"},{"step":"labelling","run":false,"reason":"nothing changed"},{"step":"takeaways","run":false,"reason":"nothing changed"},{"step":"overview","run":true,"reason":"previous data not found"},{"step":"translation","run":true,"reason":"some dependent steps will re-run: overview"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: overview, translation"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"completed","start_time":"2024-11-20T11:30:08.955241","completed_jobs":[{"step":"overview","completed":"2024-11-20T11:30:11.333341","duration":2.374369,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_openai\n\n\ndef overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config[\"overview\"][\"prompt\"]\n    model = config[\"overview\"][\"model\"]\n\n    ids = labels[\"cluster-id\"].to_list()\n    takeaways.set_index(\"cluster-id\", inplace=True)\n    labels.set_index(\"cluster-id\", inplace=True)\n\n    input = \"\"\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id][\"takeaways\"] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n\n    with open(path, \"w\") as file:\n        file.write(response)\n","prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","model":"gpt-3.5-turbo"}},{"step":"translation","completed":"2024-11-20T11:30:11.336339","duration":0.001459,"params":{"languages":[],"flags":[],"source_code":"import json\n\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom tqdm import tqdm\n\nfrom utils import messages\n\nJAPANESE_UI_MAP = {\n    \"Argument\": \"議論\",\n    # \"Original comment\": \"元のコメント\",\n    \"Representative arguments\": \"代表的な議論\",\n    \"Open full-screen map\": \"全画面地図を開く\",\n    \"Back to report\": \"レポートに戻る\",\n    \"Hide labels\": \"ラベルを非表示にする\",\n    \"Show labels\": \"ラベルを表示\",\n    \"Show filters\": \"フィルターを表示\",\n    \"Hide filters\": \"フィルターを非表示\",\n    \"Min. votes\": \"最小投票数\",\n    \"Consensus\": \"コンセンサス\",\n    \"Showing\": \"表示中\",\n    \"arguments\": \"議論\",\n    \"Reset zoom\": \"ズームをリセット\",\n    \"Click anywhere on the map to close this\": \"このメッセージを閉じるには地図のどこかをクリックしてください\",\n    \"Click on the dot for details\": \"詳細を見るには点をクリックしてください\",\n    \"agree\": \"同意する\",\n    \"disagree\": \"同意しない\",\n    \"Language\": \"言語\",\n    \"English\": \"英語\",\n    \"of total\": \"合計\",\n    \"Overview\": \"分析結果の概要\",\n    \"Cluster analysis\": \"クラスター分析\",\n    \"Representative comments\": \"コメント例\",\n    \"Introduction\": \"導入\",\n    \"Clusters\": \"クラスター\",\n    \"Appendix\": \"付録\",\n    \"This report was generated using an AI pipeline that consists of the following steps\": \"このレポートは、以下のステップで構成されるAIパイプラインを使用して生成されました\",\n    \"Step\": \"ステップ\",\n    \"extraction\": \"抽出\",\n    \"show code\": \"コードを表示\",\n    \"hide code\": \"コードを非表示\",\n    \"show prompt\": \"プロンプトを表示\",\n    \"hide prompt\": \"プロンプトを非表示\",\n    \"embedding\": \"埋め込み\",\n    \"clustering\": \"クラスタリング\",\n    \"labelling\": \"ラベリング\",\n    \"takeaways\": \"まとめ\",\n    \"overview\": \"概要\",\n}\n\n\ndef translation(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, \"w\") as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\n        \"Argument\",\n        \"Original comment\",\n        \"Representative arguments\",\n        \"Open full-screen map\",\n        \"Back to report\",\n        \"Hide labels\",\n        \"Show labels\",\n        \"Show filters\",\n        \"Hide filters\",\n        \"Min. votes\",\n        \"Consensus\",\n        \"Showing\",\n        \"arguments\",\n        \"Reset zoom\",\n        \"Click anywhere on the map to close this\",\n        \"Click on the dot for details\",\n        \"agree\",\n        \"disagree\",\n        \"Language\",\n        \"English\",\n        \"arguments\",\n        \"of total\",\n        \"Overview\",\n        \"Cluster analysis\",\n        \"Representative comments\",\n        \"Introduction\",\n        \"Clusters\",\n        \"Appendix\",\n        \"This report was generated using an AI pipeline that consists of the following steps\",\n        \"Step\",\n        \"extraction\",\n        \"show code\",\n        \"hide code\",\n        \"show prompt\",\n        \"hide prompt\",\n        \"embedding\",\n        \"clustering\",\n        \"labelling\",\n        \"takeaways\",\n        \"overview\",\n    ]\n\n    japanese_ui = [JAPANESE_UI_MAP[key] for key in UI_copy]\n    arg_list = (\n        arguments[\"argument\"].to_list()\n        + labels[\"label\"].to_list()\n        + japanese_ui\n        + languages\n    )\n\n    if \"name\" in config:\n        arg_list.append(config[\"name\"])\n    if \"question\" in config:\n        arg_list.append(config[\"question\"])\n\n    prompt_file = config.get(\"translation_prompt\", \"default\")\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config[\"model\"]\n\n    config[\"translation_prompt\"] = prompt\n\n    translations = [\n        translate_lang(arg_list, 10, prompt, lang, model) for lang in languages\n    ]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways[\"takeaways\"].to_list()\n    long_arg_list.append(overview)\n    if \"intro\" in config:\n        long_arg_list.append(config[\"intro\"])\n\n    long_translations = [\n        translate_lang(long_arg_list, 1, prompt, lang, model) for lang in languages\n    ]\n\n    for i, id in enumerate(arg_list):\n        print(\"i, id\", i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i : i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if i < len(parsed):\n                    print(\"Response:\", parsed[i])\n            if len(batch) > 1:\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(\n                    batch[:mid], lang_prompt, model, retries - 1\n                ) + translate_batch(batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nあなたはプロの翻訳者です。\n英語で書かれた単語と文章のリストを受け取ります。\n同じリストを同じ順番で、日本語に翻訳して返してください。ただし、もし文章が日本語で書かれている場合は元の文をそのまま返してください。\n元のリストと同じ長さの文字列の有効なJSONリストを返すようにしてください。\n\n","model":"gpt-3.5-turbo"}},{"step":"aggregation","completed":"2024-11-20T11:30:11.349265","duration":0.011095,"params":{"include_minor":true,"sampling_num":5000,"title_in_map":null,"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\ndef create_custom_intro(config, total_sampled_num: int):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n分析対象となったデータの件数は{input_count}件で、これらのデータに対してOpenAI APIを用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(\n        intro=intro, input_count=input_count, args_count=args_count\n    )\n\n    if total_sampled_num < args_count:\n        extra_intro = \"なお、クラスタ分析には前述の{args_count}件のデータを用いているが、本ページではそのうち{total_sampled_num}件のデータをサンプリングして可視化している。\".format(\n            args_count=args_count, total_sampled_num=total_sampled_num\n        )\n        custom_intro += extra_intro\n    custom_intro += \"一部、AIによる分析結果の中で、事実と異なる内容については削除を行った。\"\n    with open(result_path, \"r\") as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\") as f:\n        json.dump(result, f, indent=2)\n\n\ndef aggregation(config):\n    path = f\"outputs/{config['output_dir']}/result.json\"\n    total_sampling_num = config[\"aggregation\"][\"sampling_num\"]\n    print(\"total sampling num:\", total_sampling_num)\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {\"\": {}},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results[\"translations\"] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index(\"cluster-id\", inplace=True)\n\n    print(\"relevant clusters score\")\n    print(clusters.sort_values(by=\"probability\", ascending=False).head(10))\n\n    clusters[\"x\"] = clusters[\"x\"].astype(float).round(4)\n    clusters[\"y\"] = clusters[\"y\"].astype(float).round(4)\n    clusters[\"probability\"] = clusters[\"probability\"].astype(float).round(1)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results[\"overview\"] = overview\n\n    # クラスタ事に可視化するデータをサンプルする\n    # 各クラスタの件数の全体での比率をもとにサンプルする\n    arguments_num = len(arguments)\n    sample_rate = min(total_sampling_num / arguments_num, 1)\n    total_sampled_num = 0\n\n    sampled_comment_ids = []\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        arg_rows = clusters[clusters[\"cluster-id\"] == cid]\n        c_arg_num = len(arg_rows)\n        sampling_num = int(c_arg_num * sample_rate)\n\n        if (\n            not config[\"aggregation\"][\"include_minor\"]\n            and sampling_num / total_sampling_num < 0.005\n        ):\n            continue\n        print(f\"sampling num: {sampling_num}\", c_arg_num, sample_rate)\n        total_sampled_num += sampling_num\n        arguments_in_cluster = []\n\n        # pickup top 5 for representative comments\n        sorted_rows = arg_rows.sort_values(by=\"probability\", ascending=False)\n        top_5 = sorted_rows.head(5)\n        for _, arg_row in top_5.head(sampling_num).iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n        results[\"clusters\"].append(\n            {\n                \"cluster\": label,\n                \"cluster_id\": str(cid),\n                \"takeaways\": takeaways.loc[cid][\"takeaways\"],\n                \"arguments\": arguments_in_cluster,\n            }\n        )\n\n        # random sampling\n        remaining = sorted_rows.iloc[5:]\n        remaining_sample_size = max(0, sampling_num - 5)\n        random_sample = remaining.sample(\n            n=min(remaining_sample_size, len(remaining)), random_state=42\n        )\n\n        for _, arg_row in random_sample.iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n    create_custom_intro(config, total_sampled_num)\n"}},{"step":"visualization","completed":"2024-11-20T11:30:18.319178","duration":6.969064,"params":{"replacements":[],"source_code":"import subprocess\n\n\ndef visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"}}],"lock_until":"2024-11-20T11:35:18.320351","current_job":"visualization","current_job_started":"2024-11-20T11:30:11.350136","previously_completed_jobs":[{"step":"extraction","completed":"2024-11-20T11:25:04.089529","duration":5.229545,"params":{"workers":3,"limit":12,"source_code":"import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    existing_arguments = set()\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                if arg not in existing_arguments:\n                    new_row = {\n                        \"arg-id\": f\"A{comment_id}_{j}\",\n                        \"comment-id\": int(comment_id),\n                        \"argument\": arg,\n                    }\n                    results = pd.concat(\n                        [results, pd.DataFrame([new_row])], ignore_index=True\n                    )\n                    existing_arguments.add(arg)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [\n            executor.submit(extract_arguments, input, prompt, model)\n            for input in list(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait(futures, timeout=30)\n\n        results = []\n\n        for future in not_done:\n            if not future.cancelled():\n                future.cancel()\n            results.append([])\n\n        for future in done:\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                logging.error(f\"Task {future} failed with error: {e}\")\n                results.append([])\n\n        return results\n\n\ndef extract_by_llm(input, prompt, model):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model, is_json=False)\n        response = (\n            COMMA_AND_SPACE_AND_RIGHT_BRACKET.sub(r\"\\1\", response)\n            .replace(\"```json\", \"\")\n            .replace(\"```\", \"\")\n        )\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        try:\n            items = [a.strip() for a in obj]\n        except Exception as e:\n            print(\"Error:\", e)\n            print(\"Input was:\", input)\n            print(\"Response was:\", response)\n            print(\"JSON was:\", obj)\n            print(\"skip\")\n            items = []\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []\n","prompt":"/system\nあなたは専門的なリサーチアシスタントで、整理された議論のデータセットを作成するお手伝いをする役割です。\n人工知能に関する公開協議を実施した状況を想定しています。一般市民から寄せられた議論の例を提示しますので、それらをより簡潔で読みやすい形に整理するお手伝いをお願いします。必要な場合は2つの別個の議論に分割することもできますが、多くの場合は1つの議論にまとめる方が望ましいでしょう。\n結果は整形されたJSON形式の文字列リストとして返してください。\n要約は必ず日本語で作成してください。\n\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n[\n\"AIテクノロジーの環境負荷削減に焦点を当てるべき\"\n]\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、一般市民を教育する協調的な取り組みが必要です。\n\n/ai\n\n[\n\"AIの能力について一般市民を教育すべき\",\n\"AIの限界と倫理的考慮事項について一般市民を教育すべき\"\n]\n\n/human\n\nAIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できます。\n\n/ai\n\n[\n\"AIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できる\"\n]\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n[\n\"AIはエネルギーグリッドを最適化して無駄と炭素排出を削減できる\"\n]","model":"gpt-3.5-turbo"}},{"step":"embedding","completed":"2024-11-20T11:25:05.463605","duration":1.371581,"params":{"source_code":"import os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom tqdm import tqdm\n\nload_dotenv(\"../../.env\")\n\nEMBEDDING_MODEL = \"text-embedding-3-large\"\n\n\ndef embed_by_openai(args):\n    if os.getenv(\"USE_AZURE\"):\n        embeds = AzureOpenAIEmbeddings(\n            model=EMBEDDING_MODEL,\n            azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n        ).embed_documents(args)\n    else:\n        embeds = OpenAIEmbeddings(model=EMBEDDING_MODEL).embed_documents(args)\n    return embeds\n\n\ndef embedding(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = embed_by_openai(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2024-11-20T11:25:11.575914","duration":6.110666,"params":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nfrom janome.tokenizer import Tokenizer\n\nSTOP_WORDS = [\n    \"の\",\n    \"に\",\n    \"は\",\n    \"を\",\n    \"た\",\n    \"が\",\n    \"で\",\n    \"て\",\n    \"と\",\n    \"し\",\n    \"れ\",\n    \"さ\",\n    \"ある\",\n    \"いる\",\n    \"も\",\n    \"する\",\n    \"から\",\n    \"な\",\n    \"こと\",\n    \"として\",\n    \"いく\",\n    \"ない\",\n]\nTOKENIZER = Tokenizer()\n\n\ndef clustering(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config[\"clustering\"][\"clusters\"]\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        min_cluster_size=clusters,\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef tokenize_japanese(text):\n    return [\n        token.surface\n        for token in TOKENIZER.tokenize(text)\n        if token.surface not in STOP_WORDS\n    ]\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module(\"sklearn.cluster\").SpectralClustering\n    HDBSCAN = import_module(\"hdbscan\").HDBSCAN\n    UMAP = import_module(\"umap\").UMAP\n    CountVectorizer = import_module(\"sklearn.feature_extraction.text\").CountVectorizer\n    BERTopic = import_module(\"bertopic\").BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    vectorizer_model = CountVectorizer(tokenizer=tokenize_japanese)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42,\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[[\"arg-id\", \"x\", \"y\", \"probability\"]]\n    result[\"cluster-id\"] = cluster_labels\n\n    return result\n"}},{"step":"labelling","completed":"2024-11-20T11:25:17.630827","duration":6.054147,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n# TODO: プロンプト設定の外部化\nBASE_SELECTION_PROMPT = \"\"\"クラスタにつけられたラベル名と、紐づくデータ点のテキストを与えるので、\nラベル名と関連度の高いテキストのidを5つ出力してください\n\n# 指示\n* ラベルと各データ点のテキストを確認した上で、関連度の高いidを出力してください\n* 出力はカンマ区切りで、スペースを含めずに5つのidを出力して下さい\n* 出力結果は人間が閲覧するので、人間が解釈しやすいテキストを選定してください\n    * 出力はWebで公開されるため過激な発言や侮辱的な発言等の閲覧者が不快感を覚えるものは選定しないでください\n    * 同様に公共放送において不適切な単語が含まれているものは選定しないでください\n* 今回の分析は衆院選における意見分析を行うために実施しているため衆院選と関連性の低いものは選定しないでください\n    * データソースはツイートであり、ハッシュタグのみのツイート等も含まれるため、それらは選定しないでください\n\n# 出力例\nA199_0,A308_0,A134_2,A134_1,A123_0\n\n# ラベル名\n{label}\n\n# 各データ点のテキスト\n{args_text}\n\"\"\"\n\n\ndef select_relevant_ids_by_llm(prompt, model=\"gpt-4o\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model)\n        selected_ids = response.strip().split(\",\")\n        return [id_str.strip() for id_str in selected_ids]\n    except Exception as e:\n        print(e)\n        return []\n\n\ndef select_representative_args(\n    cluster_args, label, cid, model=\"gpt-4o\", sampling_num=50\n):\n    arg_rows = cluster_args[cluster_args[\"cluster-id\"] == cid].sort_values(\n        by=\"probability\", ascending=False\n    )\n    # hdbscanのクラスタにおける所属確率(probability)が高い順に取得し、代表コメントの候補とする\n    top_rows = arg_rows.head(sampling_num)\n    args_text = \"\\n\".join(\n        [\n            f\"{row['arg-id']}: {row['argument']}\"\n            for _, (_, row) in enumerate(top_rows.iterrows())\n        ]\n    )\n    prompt = BASE_SELECTION_PROMPT.format(label=label, args_text=args_text)\n    selected_ids = select_relevant_ids_by_llm(prompt, model)\n    return selected_ids\n\n\ndef update_cluster_probability(config, arguments, clusters, labels):\n    cluster_args = arguments.merge(clusters, on=\"arg-id\", how=\"left\")\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        selected_ids = select_representative_args(cluster_args, label, cid)\n        for id in selected_ids:\n            mask = cluster_args[\"arg-id\"] == id\n            clusters.loc[mask, \"probability\"] += 100\n    clusters.to_csv(f\"outputs/{config['output_dir']}/clusters.csv\", index=False)\n\n\ndef labelling(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"labelling\"][\"sample_size\"]\n    prompt = config[\"labelling\"][\"prompt\"]\n    model = config[\"labelling\"][\"model\"]\n\n    question = config[\"question\"]\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n\n        args_ids_outside = clusters[clusters[\"cluster-id\"] != cluster_id][\n            \"arg-id\"\n        ].values\n        args_ids_outside = np.random.choice(\n            args_ids_outside,\n            size=min(len(args_ids_outside), sample_size),\n            replace=False,\n        )\n        args_sample_outside = arguments[arguments[\"arg-id\"].isin(args_ids_outside)][\n            \"argument\"\n        ].values\n\n        label = generate_label(\n            question, args_sample, args_sample_outside, prompt, model\n        )\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"label\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n    update_cluster_probability(config, arguments, clusters, results)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    outside = \"\\n * \" + \"\\n * \".join(args_sample_outside)\n    inside = \"\\n * \" + \"\\n * \".join(args_sample)\n    input = (\n        f\"質問:\\n{question}\\n\\n\"\n        + f\"クラスタ外部の意見:\\n{outside}\\n\"\n        + f\"クラスタ内部の意見:\\n{inside}\"\n    )\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n","prompt":"/system \n\nクラスタ分析の結果を与えるので、クラスタにふさわしいラベルを生成してください。議論から出た意見・要望・批判、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えるのでクラスターを要約する1つのカテゴリーラベルを作成してください。\n\n質問からすでに明らかな文脈は含めない（例えば、相談の質問が「フランスでどのような課題に直面しているか」のようなものであれば、クラスターのラベルに「フランスで」と繰り返す必要はない）。\n\n今回の分析トピックは衆議院議員選挙であり、明らかに衆院選と無関係な内容は出力に含めないでください。\nラベルは非常に簡潔でなければならず、クラスターとその外側にある内容を区別するのに十分な正確さでなければならない。\nラベル名は、必ず日本語で記述してください。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定に際してEUはどのような対処をすべきだと思いますか？\n\n関心のあるクラスター以外の提案と要望の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限されないようにすべき。\n * 国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を軽減すべき。\n * 環境基準における協力を維持し、気候変動と闘う努力を向上すべき。\n * 相互医療協定の中断せずに、患者ケアを減らさないようにすべき。\n * Brexit関連の変更により、家族の居住権や市民権の申請を複雑にしないようにすべき。\n * 英国との共同研究機会を維持し、研究の課題に取り組む世界的な取り組みを維持すべき。\n * EUの文化助成プログラムからの除外の影響を減らし、創造的なプロジェクトを制限しないようにすべき。\n * EUの資金提供の喪失の影響を減らし、慈善活動やコミュニティ支援が後退しないようにしてほしい。\n * 消費者保護の弱体化させず、国境を越えた紛争解決にコミットしてほしい。\n * 英国のプロの音楽家のEU諸国ツアーを制限せず、キャリアに影響を与えないでほしい。\n\nクラスター内部での提案の例\n\n * Brexitによるサプライチェーンへの影響をとどめ、企業にとってコスト増と納期遅延を回避すべき。\n * ブレグジットによる市場の変動や投資・退職金の不確実性を減らしてほしい。\n * 新たな関税や通関手続きを考慮し、英国は輸出業者として利益率の低下に対処すべき。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転せずに、雇用を失わないようにしてほしい。\n * 英国は輸入品価格の高騰による生活費の増加に対処してほしい。\n * 英国のハイテク産業への投資を維持し、技術革新と雇用機会を保つべき。\n * 新たなビザ規制による観光客の減少に備え、新たな接客業への刺激策を考えるべき。\n * ポンド価値の下落により購買力が低下に備え、旅費の増加に対処してほしい。\n\n\n/ai \n\n財務上のマイナス影響への対処を考えるべき\n","model":"gpt-3.5-turbo"}},{"step":"takeaways","completed":"2024-11-20T11:25:24.617977","duration":6.983679,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n\ndef takeaways(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"takeaways\"][\"sample_size\"]\n    prompt = config[\"takeaways\"][\"prompt\"]\n    model = config[\"takeaways\"][\"model\"]\n\n    model = config.get(\"model_takeaways\", config.get(\"model\", \"gpt3.5-turbo\"))\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"takeaways\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    input = \"\\n\".join(args_sample)\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n","prompt":"/system \n\nあなたはプロのリサーチ・アシスタントで、私の仕事を手伝うことがあなたの仕事です。\nX上のポストのリストを与えるので、それらの主要なポイントを一つか二つの段落でまとめてください。\n応答テキストは、簡潔で、短く、読みやすい文章にしてください。\n出力は必ず日本語で行ってください。\n \n/human\n\n[\n  \"私は銃暴力が我々の社会における深刻な公衆衛生の危機であると強く信じています。\",\n  \"この問題を包括的な銃規制によって緊急に対処する必要があります。\",\n  \"私は全ての銃購入者に対するユニバーサル背景調査の実施を支持しています。\",\n  \"私はアサルト武器と大容量マガジンの禁止に賛成です。\",\n  \"違法な銃の密売を防ぐために、より厳しい規制を求めています。\",\n  \"銃購入の過程で精神的健康評価を必須にすべきだと主張しています。\"\n]\n/ai \n\n参加者たちは、銃暴力が深刻な社会問題であると認識し、これに対処するために包括的な銃規制の導入を強く求めました。具体的には、全ての銃購入者に対する背景調査の義務化や、アサルト武器と大容量マガジンの禁止、違法な銃取引の取り締まり強化、そして銃購入時の精神健康評価を優先的に導入すべきだとしています。\n","model":"gpt-3.5-turbo"}}],"end_time":"2024-11-20T11:30:18.320348"},"embedding":{"source_code":"import os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom tqdm import tqdm\n\nload_dotenv(\"../../.env\")\n\nEMBEDDING_MODEL = \"text-embedding-3-large\"\n\n\ndef embed_by_openai(args):\n    if os.getenv(\"USE_AZURE\"):\n        embeds = AzureOpenAIEmbeddings(\n            model=EMBEDDING_MODEL,\n            azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n        ).embed_documents(args)\n    else:\n        embeds = OpenAIEmbeddings(model=EMBEDDING_MODEL).embed_documents(args)\n    return embeds\n\n\ndef embedding(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = embed_by_openai(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n# TODO: プロンプト設定の外部化\nBASE_SELECTION_PROMPT = \"\"\"クラスタにつけられたラベル名と、紐づくデータ点のテキストを与えるので、\nラベル名と関連度の高いテキストのidを5つ出力してください\n\n# 指示\n* ラベルと各データ点のテキストを確認した上で、関連度の高いidを出力してください\n* 出力はカンマ区切りで、スペースを含めずに5つのidを出力して下さい\n* 出力結果は人間が閲覧するので、人間が解釈しやすいテキストを選定してください\n\n# 出力例\nA199_0,A308_0,A134_2,A134_1,A123_0\n\n# ラベル名\n{label}\n\n# 各データ点のテキスト\n{args_text}\n\"\"\"\n\n\ndef select_relevant_ids_by_llm(prompt, model=\"gpt-4o\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model)\n        selected_ids = response.strip().split(\",\")\n        return [id_str.strip() for id_str in selected_ids]\n    except Exception as e:\n        print(e)\n        return []\n\n\ndef select_representative_args(\n    cluster_args, label, cid, model=\"gpt-4o\", sampling_num=50\n):\n    arg_rows = cluster_args[cluster_args[\"cluster-id\"] == cid].sort_values(\n        by=\"probability\", ascending=False\n    )\n    # hdbscanのクラスタにおける所属確率(probability)が高い順に取得し、代表コメントの候補とする\n    top_rows = arg_rows.head(sampling_num)\n    args_text = \"\\n\".join(\n        [\n            f\"{row['arg-id']}: {row['argument']}\"\n            for _, (_, row) in enumerate(top_rows.iterrows())\n        ]\n    )\n    prompt = BASE_SELECTION_PROMPT.format(label=label, args_text=args_text)\n    selected_ids = select_relevant_ids_by_llm(prompt, model)\n    return selected_ids\n\n\ndef update_cluster_probability(config, arguments, clusters, labels):\n    cluster_args = arguments.merge(clusters, on=\"arg-id\", how=\"left\")\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        selected_ids = select_representative_args(cluster_args, label, cid)\n        for id in selected_ids:\n            mask = cluster_args[\"arg-id\"] == id\n            clusters.loc[mask, \"probability\"] += 100\n    clusters.to_csv(f\"outputs/{config['output_dir']}/clusters.csv\", index=False)\n\n\ndef labelling(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"labelling\"][\"sample_size\"]\n    prompt = config[\"labelling\"][\"prompt\"]\n    model = config[\"labelling\"][\"model\"]\n\n    question = config[\"question\"]\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n\n        args_ids_outside = clusters[clusters[\"cluster-id\"] != cluster_id][\n            \"arg-id\"\n        ].values\n        args_ids_outside = np.random.choice(\n            args_ids_outside,\n            size=min(len(args_ids_outside), sample_size),\n            replace=False,\n        )\n        args_sample_outside = arguments[arguments[\"arg-id\"].isin(args_ids_outside)][\n            \"argument\"\n        ].values\n\n        label = generate_label(\n            question, args_sample, args_sample_outside, prompt, model\n        )\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"label\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n    update_cluster_probability(config, arguments, clusters, results)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    outside = \"\\n * \" + \"\\n * \".join(args_sample_outside)\n    inside = \"\\n * \" + \"\\n * \".join(args_sample)\n    input = (\n        f\"質問:\\n{question}\\n\\n\"\n        + f\"クラスタ外部の意見:\\n{outside}\\n\"\n        + f\"クラスタ内部の意見:\\n{inside}\"\n    )\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n","prompt":"/system \n\nクラスタ分析の結果を与えるので、クラスタにふさわしいラベルを生成してください。議論から出た意見・要望・批判、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えるのでクラスターを要約する1つのカテゴリーラベルを作成してください。\n\n質問からすでに明らかな文脈は含めない（例えば、相談の質問が「フランスでどのような課題に直面しているか」のようなものであれば、クラスターのラベルに「フランスで」と繰り返す必要はない）。\n\nラベルは非常に簡潔でなければならず、クラスターとその外側にある内容を区別するのに十分な正確さでなければならない。\nラベル名は、必ず日本語で記述してください。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定に際してEUはどのような対処をすべきだと思いますか？\n\n関心のあるクラスター以外の提案と要望の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限されないようにすべき。\n * 国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を軽減すべき。\n * 環境基準における協力を維持し、気候変動と闘う努力を向上すべき。\n * 相互医療協定の中断せずに、患者ケアを減らさないようにすべき。\n * Brexit関連の変更により、家族の居住権や市民権の申請を複雑にしないようにすべき。\n * 英国との共同研究機会を維持し、研究の課題に取り組む世界的な取り組みを維持すべき。\n * EUの文化助成プログラムからの除外の影響を減らし、創造的なプロジェクトを制限しないようにすべき。\n * EUの資金提供の喪失の影響を減らし、慈善活動やコミュニティ支援が後退しないようにしてほしい。\n * 消費者保護の弱体化させず、国境を越えた紛争解決にコミットしてほしい。\n * 英国のプロの音楽家のEU諸国ツアーを制限せず、キャリアに影響を与えないでほしい。\n\nクラスター内部での提案の例\n\n * Brexitによるサプライチェーンへの影響をとどめ、企業にとってコスト増と納期遅延を回避すべき。\n * ブレグジットによる市場の変動や投資・退職金の不確実性を減らしてほしい。\n * 新たな関税や通関手続きを考慮し、英国は輸出業者として利益率の低下に対処すべき。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転せずに、雇用を失わないようにしてほしい。\n * 英国は輸入品価格の高騰による生活費の増加に対処してほしい。\n * 英国のハイテク産業への投資を維持し、技術革新と雇用機会を保つべき。\n * 新たなビザ規制による観光客の減少に備え、新たな接客業への刺激策を考えるべき。\n * ポンド価値の下落により購買力が低下に備え、旅費の増加に対処してほしい。\n\n\n/ai \n\n財務上のマイナス影響への対処を考えるべき\n","model":"gpt-3.5-turbo"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n\ndef takeaways(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"takeaways\"][\"sample_size\"]\n    prompt = config[\"takeaways\"][\"prompt\"]\n    model = config[\"takeaways\"][\"model\"]\n\n    model = config.get(\"model_takeaways\", config.get(\"model\", \"gpt3.5-turbo\"))\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"takeaways\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    input = \"\\n\".join(args_sample)\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n","prompt":"/system \n\nあなたはプロのリサーチ・アシスタントで、私の仕事を手伝うことがあなたの仕事です。\nX上のポストのリストを与えるので、それらの主要なポイントを一つか二つの段落でまとめてください。\n応答テキストは、簡潔で、短く、読みやすい文章にしてください。\n出力は必ず日本語で行ってください。\n \n/human\n\n[\n  \"私は銃暴力が我々の社会における深刻な公衆衛生の危機であると強く信じています。\",\n  \"この問題を包括的な銃規制によって緊急に対処する必要があります。\",\n  \"私は全ての銃購入者に対するユニバーサル背景調査の実施を支持しています。\",\n  \"私はアサルト武器と大容量マガジンの禁止に賛成です。\",\n  \"違法な銃の密売を防ぐために、より厳しい規制を求めています。\",\n  \"銃購入の過程で精神的健康評価を必須にすべきだと主張しています。\"\n]\n/ai \n\n参加者たちは、銃暴力が深刻な社会問題であると認識し、これに対処するために包括的な銃規制の導入を強く求めました。具体的には、全ての銃購入者に対する背景調査の義務化や、アサルト武器と大容量マガジンの禁止、違法な銃取引の取り締まり強化、そして銃購入時の精神健康評価を優先的に導入すべきだとしています。\n","model":"gpt-3.5-turbo"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_openai\n\n\ndef overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config[\"overview\"][\"prompt\"]\n    model = config[\"overview\"][\"model\"]\n\n    ids = labels[\"cluster-id\"].to_list()\n    takeaways.set_index(\"cluster-id\", inplace=True)\n    labels.set_index(\"cluster-id\", inplace=True)\n\n    input = \"\"\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id][\"takeaways\"] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n\n    with open(path, \"w\") as file:\n        file.write(response)\n","prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","model":"gpt-3.5-turbo"},"translation":{"languages":[],"flags":[],"source_code":"import json\n\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom tqdm import tqdm\n\nfrom utils import messages\n\nJAPANESE_UI_MAP = {\n    \"Argument\": \"議論\",\n    # \"Original comment\": \"元のコメント\",\n    \"Representative arguments\": \"代表的な議論\",\n    \"Open full-screen map\": \"全画面地図を開く\",\n    \"Back to report\": \"レポートに戻る\",\n    \"Hide labels\": \"ラベルを非表示にする\",\n    \"Show labels\": \"ラベルを表示\",\n    \"Show filters\": \"フィルターを表示\",\n    \"Hide filters\": \"フィルターを非表示\",\n    \"Min. votes\": \"最小投票数\",\n    \"Consensus\": \"コンセンサス\",\n    \"Showing\": \"表示中\",\n    \"arguments\": \"議論\",\n    \"Reset zoom\": \"ズームをリセット\",\n    \"Click anywhere on the map to close this\": \"このメッセージを閉じるには地図のどこかをクリックしてください\",\n    \"Click on the dot for details\": \"詳細を見るには点をクリックしてください\",\n    \"agree\": \"同意する\",\n    \"disagree\": \"同意しない\",\n    \"Language\": \"言語\",\n    \"English\": \"英語\",\n    \"of total\": \"合計\",\n    \"Overview\": \"分析結果の概要\",\n    \"Cluster analysis\": \"クラスター分析\",\n    \"Representative comments\": \"コメント例\",\n    \"Introduction\": \"導入\",\n    \"Clusters\": \"クラスター\",\n    \"Appendix\": \"付録\",\n    \"This report was generated using an AI pipeline that consists of the following steps\": \"このレポートは、以下のステップで構成されるAIパイプラインを使用して生成されました\",\n    \"Step\": \"ステップ\",\n    \"extraction\": \"抽出\",\n    \"show code\": \"コードを表示\",\n    \"hide code\": \"コードを非表示\",\n    \"show prompt\": \"プロンプトを表示\",\n    \"hide prompt\": \"プロンプトを非表示\",\n    \"embedding\": \"埋め込み\",\n    \"clustering\": \"クラスタリング\",\n    \"labelling\": \"ラベリング\",\n    \"takeaways\": \"まとめ\",\n    \"overview\": \"概要\",\n}\n\n\ndef translation(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, \"w\") as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\n        \"Argument\",\n        \"Original comment\",\n        \"Representative arguments\",\n        \"Open full-screen map\",\n        \"Back to report\",\n        \"Hide labels\",\n        \"Show labels\",\n        \"Show filters\",\n        \"Hide filters\",\n        \"Min. votes\",\n        \"Consensus\",\n        \"Showing\",\n        \"arguments\",\n        \"Reset zoom\",\n        \"Click anywhere on the map to close this\",\n        \"Click on the dot for details\",\n        \"agree\",\n        \"disagree\",\n        \"Language\",\n        \"English\",\n        \"arguments\",\n        \"of total\",\n        \"Overview\",\n        \"Cluster analysis\",\n        \"Representative comments\",\n        \"Introduction\",\n        \"Clusters\",\n        \"Appendix\",\n        \"This report was generated using an AI pipeline that consists of the following steps\",\n        \"Step\",\n        \"extraction\",\n        \"show code\",\n        \"hide code\",\n        \"show prompt\",\n        \"hide prompt\",\n        \"embedding\",\n        \"clustering\",\n        \"labelling\",\n        \"takeaways\",\n        \"overview\",\n    ]\n\n    japanese_ui = [JAPANESE_UI_MAP[key] for key in UI_copy]\n    arg_list = (\n        arguments[\"argument\"].to_list()\n        + labels[\"label\"].to_list()\n        + japanese_ui\n        + languages\n    )\n\n    if \"name\" in config:\n        arg_list.append(config[\"name\"])\n    if \"question\" in config:\n        arg_list.append(config[\"question\"])\n\n    prompt_file = config.get(\"translation_prompt\", \"default\")\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config[\"model\"]\n\n    config[\"translation_prompt\"] = prompt\n\n    translations = [\n        translate_lang(arg_list, 10, prompt, lang, model) for lang in languages\n    ]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways[\"takeaways\"].to_list()\n    long_arg_list.append(overview)\n    if \"intro\" in config:\n        long_arg_list.append(config[\"intro\"])\n\n    long_translations = [\n        translate_lang(long_arg_list, 1, prompt, lang, model) for lang in languages\n    ]\n\n    for i, id in enumerate(arg_list):\n        print(\"i, id\", i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i : i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if i < len(parsed):\n                    print(\"Response:\", parsed[i])\n            if len(batch) > 1:\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(\n                    batch[:mid], lang_prompt, model, retries - 1\n                ) + translate_batch(batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nあなたはプロの翻訳者です。\n英語で書かれた単語と文章のリストを受け取ります。\n同じリストを同じ順番で、日本語に翻訳して返してください。ただし、もし文章が日本語で書かれている場合は元の文をそのまま返してください。\n元のリストと同じ長さの文字列の有効なJSONリストを返すようにしてください。\n\n","model":"gpt-3.5-turbo"},"aggregation":{"include_minor":true,"sampling_num":5000,"title_in_map":null,"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\ndef create_custom_intro(config, total_sampled_num: int):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n分析対象となったデータの件数は{input_count}件で、これらのデータに対してOpenAI APIを用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(\n        intro=intro, input_count=input_count, args_count=args_count\n    )\n\n    if total_sampled_num < args_count:\n        extra_intro = \"なお、クラスタ分析には前述の{args_count}件のデータを用いているが、本ページではそのうち{total_sampled_num}件のデータをサンプリングして可視化している。\".format(\n            args_count=args_count, total_sampled_num=total_sampled_num\n        )\n        custom_intro += extra_intro\n    custom_intro += \"一部、AIによる分析結果の中で、事実と異なる内容については削除を行った。\"\n    with open(result_path, \"r\") as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\") as f:\n        json.dump(result, f, indent=2)\n\n\ndef aggregation(config):\n    path = f\"outputs/{config['output_dir']}/result.json\"\n    total_sampling_num = config[\"aggregation\"][\"sampling_num\"]\n    print(\"total sampling num:\", total_sampling_num)\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {\"\": {}},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results[\"translations\"] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index(\"cluster-id\", inplace=True)\n\n    print(\"relevant clusters score\")\n    print(clusters.sort_values(by=\"probability\", ascending=False).head(10))\n\n    clusters[\"x\"] = clusters[\"x\"].astype(float).round(4)\n    clusters[\"y\"] = clusters[\"y\"].astype(float).round(4)\n    clusters[\"probability\"] = clusters[\"probability\"].astype(float).round(1)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results[\"overview\"] = overview\n\n    # クラスタ事に可視化するデータをサンプルする\n    # 各クラスタの件数の全体での比率をもとにサンプルする\n    arguments_num = len(arguments)\n    sample_rate = min(total_sampling_num / arguments_num, 1)\n    total_sampled_num = 0\n\n    sampled_comment_ids = []\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        arg_rows = clusters[clusters[\"cluster-id\"] == cid]\n        c_arg_num = len(arg_rows)\n        sampling_num = int(c_arg_num * sample_rate)\n\n        if (\n            not config[\"aggregation\"][\"include_minor\"]\n            and sampling_num / total_sampling_num < 0.005\n        ):\n            continue\n        print(f\"sampling num: {sampling_num}\", c_arg_num, sample_rate)\n        total_sampled_num += sampling_num\n        arguments_in_cluster = []\n\n        # pickup top 5 for representative comments\n        sorted_rows = arg_rows.sort_values(by=\"probability\", ascending=False)\n        top_5 = sorted_rows.head(5)\n        for _, arg_row in top_5.head(sampling_num).iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n        results[\"clusters\"].append(\n            {\n                \"cluster\": label,\n                \"cluster_id\": str(cid),\n                \"takeaways\": takeaways.loc[cid][\"takeaways\"],\n                \"arguments\": arguments_in_cluster,\n            }\n        )\n\n        # random sampling\n        remaining = sorted_rows.iloc[5:]\n        remaining_sample_size = max(0, sampling_num - 5)\n        random_sample = remaining.sample(\n            n=min(remaining_sample_size, len(remaining)), random_state=42\n        )\n\n        for _, arg_row in random_sample.iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n    create_custom_intro(config, total_sampled_num)\n"},"visualization":{"replacements":[],"source_code":"import subprocess\n\n\ndef visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":false,"reason":"nothing changed"},{"step":"embedding","run":false,"reason":"nothing changed"},{"step":"clustering","run":false,"reason":"nothing changed"},{"step":"labelling","run":true,"reason":"some parameters changed: prompt"},{"step":"takeaways","run":false,"reason":"nothing changed"},{"step":"overview","run":true,"reason":"previous data not found"},{"step":"translation","run":true,"reason":"some dependent steps will re-run: labelling, overview"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: labelling, overview, translation"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"running","start_time":"2024-11-20T11:33:15.235265","completed_jobs":[{"step":"labelling","completed":"2024-11-20T11:33:19.609353","duration":4.372101,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n# TODO: プロンプト設定の外部化\nBASE_SELECTION_PROMPT = \"\"\"クラスタにつけられたラベル名と、紐づくデータ点のテキストを与えるので、\nラベル名と関連度の高いテキストのidを5つ出力してください\n\n# 指示\n* ラベルと各データ点のテキストを確認した上で、関連度の高いidを出力してください\n* 出力はカンマ区切りで、スペースを含めずに5つのidを出力して下さい\n* 出力結果は人間が閲覧するので、人間が解釈しやすいテキストを選定してください\n\n# 出力例\nA199_0,A308_0,A134_2,A134_1,A123_0\n\n# ラベル名\n{label}\n\n# 各データ点のテキスト\n{args_text}\n\"\"\"\n\n\ndef select_relevant_ids_by_llm(prompt, model=\"gpt-4o\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model)\n        selected_ids = response.strip().split(\",\")\n        return [id_str.strip() for id_str in selected_ids]\n    except Exception as e:\n        print(e)\n        return []\n\n\ndef select_representative_args(\n    cluster_args, label, cid, model=\"gpt-4o\", sampling_num=50\n):\n    arg_rows = cluster_args[cluster_args[\"cluster-id\"] == cid].sort_values(\n        by=\"probability\", ascending=False\n    )\n    # hdbscanのクラスタにおける所属確率(probability)が高い順に取得し、代表コメントの候補とする\n    top_rows = arg_rows.head(sampling_num)\n    args_text = \"\\n\".join(\n        [\n            f\"{row['arg-id']}: {row['argument']}\"\n            for _, (_, row) in enumerate(top_rows.iterrows())\n        ]\n    )\n    prompt = BASE_SELECTION_PROMPT.format(label=label, args_text=args_text)\n    selected_ids = select_relevant_ids_by_llm(prompt, model)\n    return selected_ids\n\n\ndef update_cluster_probability(config, arguments, clusters, labels):\n    cluster_args = arguments.merge(clusters, on=\"arg-id\", how=\"left\")\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        selected_ids = select_representative_args(cluster_args, label, cid)\n        for id in selected_ids:\n            mask = cluster_args[\"arg-id\"] == id\n            clusters.loc[mask, \"probability\"] += 100\n    clusters.to_csv(f\"outputs/{config['output_dir']}/clusters.csv\", index=False)\n\n\ndef labelling(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"labelling\"][\"sample_size\"]\n    prompt = config[\"labelling\"][\"prompt\"]\n    model = config[\"labelling\"][\"model\"]\n\n    question = config[\"question\"]\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n\n        args_ids_outside = clusters[clusters[\"cluster-id\"] != cluster_id][\n            \"arg-id\"\n        ].values\n        args_ids_outside = np.random.choice(\n            args_ids_outside,\n            size=min(len(args_ids_outside), sample_size),\n            replace=False,\n        )\n        args_sample_outside = arguments[arguments[\"arg-id\"].isin(args_ids_outside)][\n            \"argument\"\n        ].values\n\n        label = generate_label(\n            question, args_sample, args_sample_outside, prompt, model\n        )\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"label\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n    update_cluster_probability(config, arguments, clusters, results)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    outside = \"\\n * \" + \"\\n * \".join(args_sample_outside)\n    inside = \"\\n * \" + \"\\n * \".join(args_sample)\n    input = (\n        f\"質問:\\n{question}\\n\\n\"\n        + f\"クラスタ外部の意見:\\n{outside}\\n\"\n        + f\"クラスタ内部の意見:\\n{inside}\"\n    )\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n","prompt":"/system \n\nクラスタ分析の結果を与えるので、クラスタにふさわしいラベルを生成してください。議論から出た意見・要望・批判、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えるのでクラスターを要約する1つのカテゴリーラベルを作成してください。\n\n質問からすでに明らかな文脈は含めない（例えば、相談の質問が「フランスでどのような課題に直面しているか」のようなものであれば、クラスターのラベルに「フランスで」と繰り返す必要はない）。\n\nラベルは非常に簡潔でなければならず、クラスターとその外側にある内容を区別するのに十分な正確さでなければならない。\nラベル名は、必ず日本語で記述してください。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定に際してEUはどのような対処をすべきだと思いますか？\n\n関心のあるクラスター以外の提案と要望の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限されないようにすべき。\n * 国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を軽減すべき。\n * 環境基準における協力を維持し、気候変動と闘う努力を向上すべき。\n * 相互医療協定の中断せずに、患者ケアを減らさないようにすべき。\n * Brexit関連の変更により、家族の居住権や市民権の申請を複雑にしないようにすべき。\n * 英国との共同研究機会を維持し、研究の課題に取り組む世界的な取り組みを維持すべき。\n * EUの文化助成プログラムからの除外の影響を減らし、創造的なプロジェクトを制限しないようにすべき。\n * EUの資金提供の喪失の影響を減らし、慈善活動やコミュニティ支援が後退しないようにしてほしい。\n * 消費者保護の弱体化させず、国境を越えた紛争解決にコミットしてほしい。\n * 英国のプロの音楽家のEU諸国ツアーを制限せず、キャリアに影響を与えないでほしい。\n\nクラスター内部での提案の例\n\n * Brexitによるサプライチェーンへの影響をとどめ、企業にとってコスト増と納期遅延を回避すべき。\n * ブレグジットによる市場の変動や投資・退職金の不確実性を減らしてほしい。\n * 新たな関税や通関手続きを考慮し、英国は輸出業者として利益率の低下に対処すべき。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転せずに、雇用を失わないようにしてほしい。\n * 英国は輸入品価格の高騰による生活費の増加に対処してほしい。\n * 英国のハイテク産業への投資を維持し、技術革新と雇用機会を保つべき。\n * 新たなビザ規制による観光客の減少に備え、新たな接客業への刺激策を考えるべき。\n * ポンド価値の下落により購買力が低下に備え、旅費の増加に対処してほしい。\n\n\n/ai \n\n財務上のマイナス影響への対処を考えるべき\n","model":"gpt-3.5-turbo"}},{"step":"overview","completed":"2024-11-20T11:33:21.671212","duration":2.06012,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_openai\n\n\ndef overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config[\"overview\"][\"prompt\"]\n    model = config[\"overview\"][\"model\"]\n\n    ids = labels[\"cluster-id\"].to_list()\n    takeaways.set_index(\"cluster-id\", inplace=True)\n    labels.set_index(\"cluster-id\", inplace=True)\n\n    input = \"\"\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id][\"takeaways\"] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n\n    with open(path, \"w\") as file:\n        file.write(response)\n","prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","model":"gpt-3.5-turbo"}},{"step":"translation","completed":"2024-11-20T11:33:21.674950","duration":0.001838,"params":{"languages":[],"flags":[],"source_code":"import json\n\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom tqdm import tqdm\n\nfrom utils import messages\n\nJAPANESE_UI_MAP = {\n    \"Argument\": \"議論\",\n    # \"Original comment\": \"元のコメント\",\n    \"Representative arguments\": \"代表的な議論\",\n    \"Open full-screen map\": \"全画面地図を開く\",\n    \"Back to report\": \"レポートに戻る\",\n    \"Hide labels\": \"ラベルを非表示にする\",\n    \"Show labels\": \"ラベルを表示\",\n    \"Show filters\": \"フィルターを表示\",\n    \"Hide filters\": \"フィルターを非表示\",\n    \"Min. votes\": \"最小投票数\",\n    \"Consensus\": \"コンセンサス\",\n    \"Showing\": \"表示中\",\n    \"arguments\": \"議論\",\n    \"Reset zoom\": \"ズームをリセット\",\n    \"Click anywhere on the map to close this\": \"このメッセージを閉じるには地図のどこかをクリックしてください\",\n    \"Click on the dot for details\": \"詳細を見るには点をクリックしてください\",\n    \"agree\": \"同意する\",\n    \"disagree\": \"同意しない\",\n    \"Language\": \"言語\",\n    \"English\": \"英語\",\n    \"of total\": \"合計\",\n    \"Overview\": \"分析結果の概要\",\n    \"Cluster analysis\": \"クラスター分析\",\n    \"Representative comments\": \"コメント例\",\n    \"Introduction\": \"導入\",\n    \"Clusters\": \"クラスター\",\n    \"Appendix\": \"付録\",\n    \"This report was generated using an AI pipeline that consists of the following steps\": \"このレポートは、以下のステップで構成されるAIパイプラインを使用して生成されました\",\n    \"Step\": \"ステップ\",\n    \"extraction\": \"抽出\",\n    \"show code\": \"コードを表示\",\n    \"hide code\": \"コードを非表示\",\n    \"show prompt\": \"プロンプトを表示\",\n    \"hide prompt\": \"プロンプトを非表示\",\n    \"embedding\": \"埋め込み\",\n    \"clustering\": \"クラスタリング\",\n    \"labelling\": \"ラベリング\",\n    \"takeaways\": \"まとめ\",\n    \"overview\": \"概要\",\n}\n\n\ndef translation(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, \"w\") as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\n        \"Argument\",\n        \"Original comment\",\n        \"Representative arguments\",\n        \"Open full-screen map\",\n        \"Back to report\",\n        \"Hide labels\",\n        \"Show labels\",\n        \"Show filters\",\n        \"Hide filters\",\n        \"Min. votes\",\n        \"Consensus\",\n        \"Showing\",\n        \"arguments\",\n        \"Reset zoom\",\n        \"Click anywhere on the map to close this\",\n        \"Click on the dot for details\",\n        \"agree\",\n        \"disagree\",\n        \"Language\",\n        \"English\",\n        \"arguments\",\n        \"of total\",\n        \"Overview\",\n        \"Cluster analysis\",\n        \"Representative comments\",\n        \"Introduction\",\n        \"Clusters\",\n        \"Appendix\",\n        \"This report was generated using an AI pipeline that consists of the following steps\",\n        \"Step\",\n        \"extraction\",\n        \"show code\",\n        \"hide code\",\n        \"show prompt\",\n        \"hide prompt\",\n        \"embedding\",\n        \"clustering\",\n        \"labelling\",\n        \"takeaways\",\n        \"overview\",\n    ]\n\n    japanese_ui = [JAPANESE_UI_MAP[key] for key in UI_copy]\n    arg_list = (\n        arguments[\"argument\"].to_list()\n        + labels[\"label\"].to_list()\n        + japanese_ui\n        + languages\n    )\n\n    if \"name\" in config:\n        arg_list.append(config[\"name\"])\n    if \"question\" in config:\n        arg_list.append(config[\"question\"])\n\n    prompt_file = config.get(\"translation_prompt\", \"default\")\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config[\"model\"]\n\n    config[\"translation_prompt\"] = prompt\n\n    translations = [\n        translate_lang(arg_list, 10, prompt, lang, model) for lang in languages\n    ]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways[\"takeaways\"].to_list()\n    long_arg_list.append(overview)\n    if \"intro\" in config:\n        long_arg_list.append(config[\"intro\"])\n\n    long_translations = [\n        translate_lang(long_arg_list, 1, prompt, lang, model) for lang in languages\n    ]\n\n    for i, id in enumerate(arg_list):\n        print(\"i, id\", i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i : i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if i < len(parsed):\n                    print(\"Response:\", parsed[i])\n            if len(batch) > 1:\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(\n                    batch[:mid], lang_prompt, model, retries - 1\n                ) + translate_batch(batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nあなたはプロの翻訳者です。\n英語で書かれた単語と文章のリストを受け取ります。\n同じリストを同じ順番で、日本語に翻訳して返してください。ただし、もし文章が日本語で書かれている場合は元の文をそのまま返してください。\n元のリストと同じ長さの文字列の有効なJSONリストを返すようにしてください。\n\n","model":"gpt-3.5-turbo"}}],"lock_until":"2024-11-20T11:38:21.676638","current_job":"aggregation","current_job_started":"2024-11-20T11:33:21.676605"}}},"__N_SSG":true}